# LLMLanguageFineTuningModifiesMathLogic
This repository presents a research on the effects of fine-tuning an LLM using a minimal dataset of a symbolic language. The results demonstrate a "logical architecture transfer" that modifies the model's reasoning and a "catastrophic forgetting" that makes it faster, but with an "optimized fragility."
